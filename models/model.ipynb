{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pandas as pd\n",
    "        \n",
    "def Columns_Remake():\n",
    "    Dataset_Remake = pd.read_csv(os.path.join(\"UK Accident Dataset\", \"Accident_Information.csv\"), encoding=\"latin1\", low_memory=False).copy()\n",
    "    Dataset_Remake.columns = [col.replace(\"_\", \" \") for col in Dataset_Remake.columns]\n",
    "    if \"Date\" in Dataset_Remake.columns:\n",
    "        Dataset_Remake[\"Date\"] = (\n",
    "            pd.to_datetime(Dataset_Remake[\"Date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "        ).dt.strftime(\"%d-%m-%Y\")\n",
    "    elif \"Time\" in Dataset_Remake.columns:\n",
    "        Dataset_Remake[\"Time\"] = (\n",
    "            pd.to_datetime(Dataset_Remake[\"Time\"], format=\"%H:%M\", errors=\"coerce\")\n",
    "        ).dt.strftime(\"%H:%M:%S\")\n",
    "        \n",
    "    Dataset_Remake.to_csv(\n",
    "        os.path.join(\"UK Accident Dataset\", \"Accident_Information_Remake.csv\"),\n",
    "        index=False,\n",
    "    )        \n",
    "\n",
    "def Remake():\n",
    "    os.makedirs(\"UK Accident Dataset\", exist_ok=True)\n",
    "    os.makedirs(\"Model\", exist_ok=True)     \n",
    "    if \"Accident_Information.csv\" in os.listdir():\n",
    "        shutil.move(\"Accident_Information.csv\", \"UK Accident Dataset\")\n",
    "        Columns_Remake()\n",
    "    return pd.read_csv(os.path.join(\"UK Accident Dataset\", \"Accident_Information_Remake.csv\"), encoding=\"latin1\", low_memory=False).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Tắt tất cả warning và info logs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Remake_Dataset import Remake\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf\n",
    "\n",
    "# Thiết lập style cho đồ thị - sửa phần này\n",
    "plt.style.use('default')  # Thay đổi từ 'seaborn' thành 'default'\n",
    "sns.set_theme()  # Cách thiết lập mới cho seaborn\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    print(\"1. Đang tải và chuẩn bị dữ liệu...\")\n",
    "    df = Remake()\n",
    "    \n",
    "    # Chọn features quan trọng\n",
    "    features = [\n",
    "        'Road Type', 'Speed limit', 'Light Conditions', \n",
    "        'Weather Conditions', 'Road Surface Conditions', \n",
    "        'Urban or Rural Area', 'Junction Detail', \n",
    "        'Junction Control', 'Pedestrian Crossing-Human Control',\n",
    "        'Pedestrian Crossing-Physical Facilities'\n",
    "    ]\n",
    "    target = 'Accident Severity'\n",
    "    \n",
    "    # Xử lý missing values\n",
    "    for col in features:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    return df, features, target\n",
    "\n",
    "def encode_features(df, features, target):\n",
    "    print(\"2. Đang mã hóa features...\")\n",
    "    X = df[features].copy()\n",
    "    encoders = {}\n",
    "    \n",
    "    # Mã hóa từng feature\n",
    "    for column in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        X[column] = le.fit_transform(X[column].astype(str))\n",
    "        encoders[column] = le\n",
    "    \n",
    "    # Mã hóa target\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(df[target])\n",
    "    \n",
    "    return X, y, encoders, le_target\n",
    "\n",
    "def train_random_forest(X, y):\n",
    "    print(\"3. Huấn luyện mô hình Random Forest...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Khởi tạo và huấn luyện mô hình\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5)\n",
    "    print(f\"\\nĐộ chính xác cross-validation: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Huấn luyện mô hình cuối cùng\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    return rf_model, X_train, X_test, y_train, y_test\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, features):\n",
    "    print(\"\\n4. Đánh giá mô hình...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Tính độ chính xác\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nĐộ chính xác của mô hình: {accuracy:.4f}\")\n",
    "    \n",
    "    # In báo cáo phân loại với zero_division=1\n",
    "    print(\"\\nBáo cáo phân loại chi tiết:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n",
    "    \n",
    "    # Vẽ confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.savefig('Model/confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Vẽ feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.savefig('Model/feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "def train_neural_network(X, y):\n",
    "    print(\"\\n5. Huấn luyện Neural Network để so sánh...\")\n",
    "    try:\n",
    "        # Chuẩn hóa dữ liệu\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Chia dữ liệu\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Chuyển đổi dữ liệu thành numpy array\n",
    "        X_train = np.array(X_train, dtype=np.float32)\n",
    "        X_test = np.array(X_test, dtype=np.float32)\n",
    "        \n",
    "        # One-hot encoding cho target\n",
    "        num_classes = len(np.unique(y))\n",
    "        y_train_cat = np.eye(num_classes)[y_train]\n",
    "        y_test_cat = np.eye(num_classes)[y_test]\n",
    "        \n",
    "        # Xây dựng model với các layer đơn giản hơn\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, activation='relu', input_dim=X.shape[1]))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        \n",
    "        # Compile với các tham số cơ bản\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Huấn luyện với batch size nhỏ hơn\n",
    "        history = model.fit(\n",
    "            X_train, y_train_cat,\n",
    "            validation_data=(X_test, y_test_cat),\n",
    "            epochs=20,\n",
    "            batch_size=32,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Vẽ learning curves\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('Model/neural_network_performance.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return model, scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nLỗi khi huấn luyện Neural Network: {str(e)}\")\n",
    "        print(\"Bỏ qua phần Neural Network và tiếp tục với Random Forest...\")\n",
    "        return None, None\n",
    "\n",
    "def main():\n",
    "    # Tải và chuẩn bị dữ liệu\n",
    "    df, features, target = load_and_prepare_data()\n",
    "    X, y, encoders, le_target = encode_features(df, features, target)\n",
    "    \n",
    "    # Huấn luyện và đánh giá Random Forest\n",
    "    rf_model, X_train, X_test, y_train, y_test = train_random_forest(X, y)\n",
    "    feature_importance = evaluate_model(rf_model, X_test, y_test, features)\n",
    "    \n",
    "    # Huấn luyện Neural Network\n",
    "    nn_model, scaler = train_neural_network(X, y)\n",
    "    \n",
    "    # Lưu các model và encoder\n",
    "    print(\"\\n6. Lưu các model và encoder...\")\n",
    "    joblib.dump(rf_model, 'Model/random_forest_model.joblib')\n",
    "    joblib.dump(encoders, 'Model/feature_encoders.joblib')\n",
    "    joblib.dump(le_target, 'Model/target_encoder.joblib')\n",
    "    \n",
    "    if nn_model is not None:\n",
    "        joblib.dump(scaler, 'Model/scaler.joblib')\n",
    "        nn_model.save('Model/neural_network_model.h5')\n",
    "    \n",
    "    print(\"\\nQuá trình huấn luyện hoàn tất!\")\n",
    "    print(\"Các file đã được lưu trong thư mục 'Model':\")\n",
    "    print(\"- Model Random Forest: random_forest_model.joblib\")\n",
    "    print(\"- Encoders: feature_encoders.joblib\")\n",
    "    print(\"- Target Encoder: target_encoder.joblib\")\n",
    "    if nn_model is not None:\n",
    "        print(\"- Neural Network: neural_network_model.h5\")\n",
    "        print(\"- Scaler: scaler.joblib\")\n",
    "        print(\"- Đồ thị Neural Network Performance: neural_network_performance.png\")\n",
    "    print(\"- Đồ thị Confusion Matrix: confusion_matrix.png\")\n",
    "    print(\"- Đồ thị Feature Importance: feature_importance.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
